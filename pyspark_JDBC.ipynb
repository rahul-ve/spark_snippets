{"cells":[{"cell_type":"markdown","source":["## JDBC - SQL Server - pyspark DF"],"metadata":{}},{"cell_type":"code","source":["\n","jdbcHostname = \"some_server\"   \n","jdbcDatabase = \"db\"\n","jdbcPort = 1433\n","jdbcUsername = dbutils.secrets.get(scope = \"\", key = \"\")     # need to specify scope and key\n","jdbcPassword = dbutils.secrets.get(scope = \"\", key = \"\")      # need to specify scope and key\n","MSSQLdriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n","jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n","connectionProperties = {\n","  \"user\" : jdbcUsername,\n","  \"password\" : jdbcPassword,\n","  \"driver\" : MSSQLdriver\n","}"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### TRY USING load()/save() methods over jdbc() method, it is easy to read!!!"],"metadata":{}},{"cell_type":"markdown","source":["### READ TO DF\n","  - https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader"],"metadata":{}},{"cell_type":"code","source":["# READ TO DF\n","\n","\n","# JDBC method\n","pushdown_query = \"(select top 1 * FROM DBtest.dbo.test_table) some_alias\"            # NOTE the alias, it is important!\n","df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\n","display(df)\n","\n","# jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)\n","df = spark.read.jdbc(url=jdbcUrl, table=\"employees\", column=\"emp_no\", lowerBound=1, upperBound=100000, numPartitions=100)\n","display(df)\n","\n","  \n","# OR\n","\n","# load method    \n","# NOTE the \"fetchsize\" option!!!!\n","# OTHER OPTION THAT CAN IMPACT SPEED IS \"numPartitions\" (for both READ and WRITE)  -- this can cause issues on the server if used inappropriately!!\n","\n","\n","#NOTE dont NEED to add any trailing or leading spaces when splitting a long line of code!!\n","\n","df = spark.read\\\n",".format(\"jdbc\")\\\n",".option(\"url\", jdbcUrl)\\\n",".option(\"driver\", MSSQLdriver)\\\n",".option(\"dbtable\", \"schema.tablename\")\\\n",".option(\"user\", \"username\")\\\n",".option(\"password\", \"password\")\\\n",".option(\"customSchema\", \"id DECIMAL(38, 0), name STRING\")\\\n",".option(\"lowerBound\", min)\\\n",".option(\"upperBound\", max)\\\n",".option(\"numPartitions\", numPartitions)\\\n",".option(\"partitionColumn\", primaryKey)\\\n",".option(\"fetchsize\", 10000)\n",".load()\n","\n","## understand bounds to load data in parallel"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# EXAMPLE  USING \"query\" INSTEAD OF \"dbtable\"\n","\n","queryREAD =  \"\"\"\n","SELECT \n","     col1\n","     ,col2\n","     ,col3\n","  FROM DBtest.dbo.test_table\n","  where col3 = 'test'\n","\"\"\"\n","\n","df = spark.read\\\n",".format(\"jdbc\")\\\n",".option(\"url\", jdbcUrl)\\\n",".option(\"driver\", MSSQLdriver)\\\n",".option(\"query\", queryREAD)\\\n",".option(\"user\", jdbcUsername)\\\n",".option(\"password\", jdbcPassword)\\\n",".option(\"fetchsize\", 10000).load()\n","  \n","display(df)\n","\n","## WITH SPACING  -- ONE space before \\  and TWO leading spaces before each subsequent line\n","\n","df = spark.read \\\n","  .format(\"jdbc\") \\\n","  .option(\"url\", jdbcUrl) \\\n","  .option(\"driver\", MSSQLdriver) \\\n","  .option(\"query\", queryREAD) \\\n","  .option(\"user\", jdbcUsername) \\\n","  .option(\"password\", jdbcPassword) \\\n","  .option(\"fetchsize\", 10000) \\\n","  .load()\n","  \n","display(df)\n","\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### WRITE FROM DF  \n","-  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter"],"metadata":{}},{"cell_type":"code","source":["# WRITE FROM DF  \n","\n","# truncate, True means same as Truncate in SQL, keeps schema!!!!\n","# truncate, False means DROP and RECREATE  the table, loses schema!!!!\n","\n","someDF.write\\\n",".mode(\"overwrite\")\\\n",".option(\"truncate\", True)\\\n",".format(\"jdbc\")\\\n",".option(\"url\", jdbcUrl)\\\n",".option(\"dbtable\", \"dbo.test_tab2\")\\\n",".option(\"user\", jdbcUsername)\\\n",".option(\"password\", jdbcPassword)\\\n",".option(\"driver\", MSSQLdriver)\\\n",".option(\"batchsize\", 10000)\\\n",".save()\n","\n","\n","# OTHER OPTIONS\n","    #  .option(\"createTableColumnTypes\", \"col1 INT, col2 VARCHAR(128), col3 VARCHAR(128)\")\n","\n","\n","\n","# jdbc method\n","  #  jdbc(url, table, mode=None, properties=None)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### REFERENCE:\n","   - https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n","   -  https://github.com/apache/spark/blob/master/examples/src/main/python/sql/datasource.py\n","    \n","### OVERWRITE/Truncate  \n"," -    https://github.com/apache/spark/pull/14086\n","  \n","    - MODE\n","      - mode â€“ specifies the behavior of the save operation when data already exists.\n","          - append: Append contents of this DataFrame to existing data.\n","          - overwrite: Overwrite existing data.\n","          - ignore: Silently ignore this operation if data already exists.\n","          - error or errorifexists (default case): Throw an exception if data already exists.\n","\n","  \n","### ALL JDBC PROPERTIES   -- READ THIS DOC!!!!!\n","  -  SEE: https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html"],"metadata":{}},{"cell_type":"code","source":["# PUSH DOWN OPTIMIZATIONS\n","  # SEE:  https://docs.databricks.com/spark/latest/data-sources/sql-databases.html#establish-connectivity-to-sql-server \n","\n","df = spark.read.jdbc(jdbcUrl, \"diamonds\", connectionProperties).select(\"carat\", \"cut\", \"price\").where(\"cut = 'Good'\").explain(true)"],"metadata":{},"outputs":[],"execution_count":10},{"source":["## PostgreSQL "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Either IP or FQDN work fine!\n","\n","jdbc_server_IP = 'xxx.xx.xx.x'\n","jdbcHostname = \"xxxxxx.postgres.database.azure.com\"   \n","jdbcDatabase = \"xxxxx\"\n","jdbcPort = 5432\n","jdbcUsername = dbutils.secrets.get(scope = \"azure-kv\", key = \"xxxxx\")\n","jdbcPassword = dbutils.secrets.get(scope = \"azure-kv\", key = \"xxxxx\")\n","Postgres_driver = \"org.postgresql.Driver\"\n","jdbcUrl = \"jdbc:postgresql://{0}:{1}/{2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n","jdbcUrl_IP = \"jdbc:postgresql://{0}:{1}/{2}\".format(jdbc_server_IP, jdbcPort, jdbcDatabase)\n","connectionProperties = {\n","  \"user\" : jdbcUsername,\n","  \"password\" : jdbcPassword,\n","  \"driver\" : Postgres_driver\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query =  \"\"\"\n","SELECT \n","     col1\n","     ,col2\n","     ,col3\n","  FROM DBtest.dbo.test_table\n","  where col3 = 'test'\n","\"\"\"\n","\n","df1 = spark.read\\\n",".format(\"jdbc\")\\\n",".option(\"url\", jdbcUrl)\\\n",".option(\"driver\", Postgres_driver)\\\n",".option(\"dbtable\", query)\\\n",".option(\"user\", jdbcUsername)\\\n",".option(\"password\", jdbcPassword)\\\n",".option(\"fetchsize\", 10000)\\\n",".option(\"numPartitions\", 8)\\\n",".load()"]},{"source":[],"cell_type":"markdown","metadata":{}}],"metadata":{"name":"pyspark_JDBC","notebookId":605722082820645},"nbformat":4,"nbformat_minor":0}