{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Azure Service Principal for Databricks automation\n",
    "\n",
    "The main motivation is to remove any user account dependency on automation workflows. Solution was to use Service Principal (SP) instead of user accounts for job ownership and triggering. **There are several ways in which SP can be setup to handle Databricks automation and below process only illustrates one specific way.**\n",
    "\n",
    "**IMPORTANT - SP credentials, PAT are sensitive information and should be treated with care. Use key vault to store and access them at runtime.**\n",
    "\n",
    "### Steps:\n",
    "\n",
    "- Service Principal needs to be assigned to \"contributor\" role for the Azure Databricks (ADB) workspace resource in Azure (This will add SP to the admins group first time it is added to ADB).\n",
    "- Add SP to ADB, will require an existing admin PAT to add SP to workspace\n",
    "- Get Azure Active Directory (AAD) OAuth2 access token using (SP) creds\n",
    "- Get Databricks PAT using AAD OAuth2 access token\n",
    "- Add PAT to Azure key vault and use this in automation, or to trigger jobs from Azure Data Factory (ADF)!\n",
    "    - i.e,  ADB_PAT = dbutils.secrets.get(scope = \"azure-kv\", key = \"sp-pat\")   \n",
    "    - **Currently ADF does not allow for OAuth2 workflow, needs PAT**  \n",
    "\n",
    "**For jobs**\n",
    "- Transfer ownership of jobs to SP\n",
    "- As SP was added to admin group (via membership to contributor role in Azure), it will have inherited \"Manage\" permission to all Clusters, access to all notebooks and secret scopes!!\n",
    "\n",
    "**Using SP without adding to admin group**\n",
    "- Don't add SP to \"contributor\" role for the Azure Databricks (ADB) workspace resource in Azure\n",
    "- Follow https://docs.microsoft.com/en-us/azure/databricks/tutorials/run-jobs-with-service-principals#--create-a-service-principal-in-azure-active-directory\n",
    "- This requires managing permissions for Clusters and Secret scopes manually\n",
    "\n",
    "\n",
    "**Ref**:\n",
    "- https://docs.microsoft.com/en-us/azure/databricks/tutorials/run-jobs-with-service-principals\n",
    "- https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/scim/scim-sp\n",
    "- https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/scim/\n",
    "- https://docs.databricks.com/dev-tools/api/latest/index.html\n",
    "- https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/service-prin-aad-token\n",
    "\n",
    "\n",
    "### Another option for ADF\n",
    "- [Using ADF \"Managed Identity\" to authenticate instead of PAT](https://techcommunity.microsoft.com/t5/azure-data-factory/azure-databricks-activities-now-support-managed-identity/ba-p/1922818)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from getpass import getpass\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADB_ADMIN_PAT = getpass()       ## Existing Admin PAT for databricks to add SP to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_secret = getpass()      ## This is the client secret received when SP/Application Was registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADB_instance = \"adb-xxxxxxxxxxxxxxxx.xx.azuredatabricks.net\"        ## Databricks instance domain name\n",
    "\n",
    "SP_name = \"xxxx\"    ## name in AD - SP/Application name\n",
    "SP_app_id =  \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"   ## client ID\n",
    "\n",
    "AZ_TENENT_ID = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"    ## az account show -> is \"homeTenantId\" value \n",
    "AZ_ADB_RESOURCE_ID = \"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d\"    ## this is common for all of databicks on azure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add SP to workspace\n",
    "## see  https://docs.microsoft.com/en-us/azure/databricks/tutorials/run-jobs-with-service-principals#--add-the-service-principal-to-the-azure-databricks-workspace\n",
    "headers_add_sp = {\n",
    "    'Content-Type': 'application/scim+json',\n",
    "    'Authorization': f'Bearer {ADB_ADMIN_PAT}'\n",
    "}\n",
    "\n",
    "payload_add_sp = {\n",
    "    \"schemas\":[\n",
    "      \"urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal\"\n",
    "    ],\n",
    "    \"applicationId\": f\"{SP_app_id}\",\n",
    "    \"displayName\": f\"{SP_name}\",\n",
    "    \"entitlements\":[\n",
    "      {\n",
    "        \"value\":\"allow-cluster-create\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "URL_ADD_SP = f'https://{ADB_instance}/api/2.0/preview/scim/v2/ServicePrincipals'\n",
    "\n",
    "resp_add_sp = requests.post(URL_ADD_SP, headers=headers_add_sp, json=payload_add_sp)\n",
    "resp_add_sp.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get AAD access token\n",
    "\n",
    "head_aad_access_token = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "}\n",
    "data_aad_access_token = {\n",
    "  'grant_type': 'client_credentials',\n",
    "  'client_id': f'{SP_app_id}',\n",
    "  'resource': f'{AZ_ADB_RESOURCE_ID}',\n",
    "  'client_secret':  f'{SP_secret}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_aad_token = requests.get(f'https://login.microsoftonline.com/{AZ_TENENT_ID}/oauth2/token', headers=head_aad_access_token, data=data_aad_access_token)\n",
    "aad_token_sp = resp_aad_token.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get a PAT using SP creds\n",
    "\n",
    "TOKEN_API_CREATE = f\"https://{ADB_instance}/api/2.0/token/create\"\n",
    "\n",
    "headers_adb = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {aad_token_sp}'\n",
    "}\n",
    "\n",
    "## skip the \"lifetime_seconds\" key to create the token without expiry!!\n",
    "data_adb = { \"lifetime_seconds\" : 86400,\n",
    "                \"comment\": \"token generated using SP aad oauth token, expires in 24hrs\" }\n",
    "\n",
    "resp_token_create = requests.post(TOKEN_API_CREATE, headers=headers_adb, json = data_adb)\n",
    "SP_PAT = resp_token_create.json()[\"token_value\"]          ## save this to azure key vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST A JOB -- TRIGGER USING SP PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_SUBMIT_URL = f\"https://{ADB_instance}/api/2.0/jobs/run-now\"\n",
    "TEST_JOB = xxxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_job = {'job_id' : TEST_JOB}\n",
    "\n",
    "head_job = {\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {SP_PAT}\"\n",
    "    }\n",
    "resp_job = requests.post(JOB_SUBMIT_URL, headers=head_job, json=data_job)\n",
    "resp_job.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:main]",
   "language": "python",
   "name": "conda-env-main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}